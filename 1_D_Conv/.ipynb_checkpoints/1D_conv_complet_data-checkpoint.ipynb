{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb26fc52-2e16-4eb4-9b50-a404d5f429b6",
   "metadata": {},
   "source": [
    "This notebook is inspired by [this](https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e464f34-d949-4efb-95e8-4b65781e84db",
   "metadata": {},
   "source": [
    "## Architecture description\n",
    "We will define the model as having two 1D CNN layers, followed by a dropout layer for regularization, then a pooling layer. It is common to define CNN layers in groups of two in order to give the model a good chance of learning features from the input data. CNNs learn very quickly, so the dropout layer is intended to help slow down the learning process and hopefully result in a better final model. The pooling layer reduces the learned features to 1/4 their size, consolidating them to only the most essential elements.\n",
    "\n",
    "After the CNN and pooling, the learned features are flattened to one long vector, which is fused with features coming from other dimensions,  and pass through a fully connected layer before the output layer used to make a prediction. The fully connected layer ideally provides a buffer between the learned features and the output with the intent of interpreting the learned features before making a prediction.\n",
    "\r",
    "\r\n",
    "For this model, we will use a standard configuration of 64 parallel feature maps and a kernel size of 3. The feature maps are the number of times the input is processed or interpreted, whereas the kernel size is the number of input time steps considered as the input sequence is read or processed onto the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c0b24903-8259-4422-85fa-a83475b48544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9ade1-c903-4c64-9803-29c70e725673",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c5bc-1983-43b5-9b96-e99b8b046c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet1D(nn.Module):\n",
    "    def __init__(self,n_features = 1, n_classes = 2, n_timesteps = 128, kernel = 5 ):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_timesteps = window_size\n",
    "        self.kernel_size = kernel\n",
    "        # with convolution\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_features, out_channels=64, kernel_size=self.kernel_size )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=self.kernel_size )\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Calculate the size of the input to the Linear layer\n",
    "        input_size = 64 * ((self.n_timesteps - self.kernel_size*2) // 2)  # Adjust based on your specific input size\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, self.n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ab786-a059-4ceb-9c12-caf55efd33ee",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "421e3df0-d7a2-4a3c-b1fa-9fe95f918266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1554</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "      <th>1558</th>\n",
       "      <th>1559</th>\n",
       "      <th>1560</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1563</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>freq</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>flash</td>\n",
       "      <td>1.009200</td>\n",
       "      <td>1.523200</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>1.152000</td>\n",
       "      <td>1.337600</td>\n",
       "      <td>1.007600</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>bandwidth</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>timeSignal</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>time_sincelast</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>ampl</td>\n",
       "      <td>115.189682</td>\n",
       "      <td>181.916809</td>\n",
       "      <td>107.035546</td>\n",
       "      <td>113.025248</td>\n",
       "      <td>204.781645</td>\n",
       "      <td>459.905485</td>\n",
       "      <td>206.880764</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>duratSec</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>frac</td>\n",
       "      <td>0.442529</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.364583</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.467249</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "      <td>phase</td>\n",
       "      <td>-0.176576</td>\n",
       "      <td>1.975381</td>\n",
       "      <td>-0.215273</td>\n",
       "      <td>0.658635</td>\n",
       "      <td>0.088833</td>\n",
       "      <td>-0.061623</td>\n",
       "      <td>-2.434454</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>freq</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>flash</td>\n",
       "      <td>1.030400</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.214400</td>\n",
       "      <td>1.011200</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>1.335600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>bandwidth</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>timeSignal</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>1.018000</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>time_sincelast</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "      <td>ampl</td>\n",
       "      <td>3.748988</td>\n",
       "      <td>5.578502</td>\n",
       "      <td>7.998470</td>\n",
       "      <td>9.371808</td>\n",
       "      <td>2.671393</td>\n",
       "      <td>2.912165</td>\n",
       "      <td>3.786173</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 1564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1               2           3           4           5     \\\n",
       "0   control     4            freq   17.400000   11.200000   30.500000   \n",
       "1   control     4           flash    1.009200    1.523200    0.793000   \n",
       "2   control     4       bandwidth    7.700000    3.600000   19.000000   \n",
       "3   control     4      timeSignal    0.098000    0.238000    0.256000   \n",
       "4   control     4  time_sincelast    0.098000    0.140000    0.018000   \n",
       "5   control     4            ampl  115.189682  181.916809  107.035546   \n",
       "6   control     4        duratSec    0.058000    0.136000    0.026000   \n",
       "7   control     4            frac    0.442529    0.321429    0.622951   \n",
       "8   control     4           phase   -0.176576    1.975381   -0.215273   \n",
       "9   control     6            freq   36.800000   19.200000   26.400000   \n",
       "10  control     6           flash    1.030400    0.960000    1.214400   \n",
       "11  control     6       bandwidth   17.400000    8.300000   10.600000   \n",
       "12  control     6      timeSignal    0.094000    0.306000    0.514000   \n",
       "13  control     6  time_sincelast    0.094000    0.212000    0.208000   \n",
       "14  control     6            ampl    3.748988    5.578502    7.998470   \n",
       "\n",
       "          6           7           8           9     ...  1554  1555  1556  \\\n",
       "0    19.200000    8.800000   22.900000    6.900000  ...   NaN   NaN   NaN   \n",
       "1     1.152000    1.337600    1.007600    1.007400  ...   NaN   NaN   NaN   \n",
       "2     7.000000    2.600000   10.700000    3.900000  ...   NaN   NaN   NaN   \n",
       "3     0.528000    0.536000    0.802000    0.850000  ...   NaN   NaN   NaN   \n",
       "4     0.272000    0.008000    0.266000    0.048000  ...   NaN   NaN   NaN   \n",
       "5   113.025248  204.781645  459.905485  206.880764  ...   NaN   NaN   NaN   \n",
       "6     0.060000    0.152000    0.044000    0.146000  ...   NaN   NaN   NaN   \n",
       "7     0.364583    0.295455    0.467249    0.565217  ...   NaN   NaN   NaN   \n",
       "8     0.658635    0.088833   -0.061623   -2.434454  ...   NaN   NaN   NaN   \n",
       "9    15.800000   22.000000   21.100000   10.600000  ...   NaN   NaN   NaN   \n",
       "10    1.011200    1.276000    1.012800    1.335600  ...   NaN   NaN   NaN   \n",
       "11    6.700000    8.000000    9.000000    3.000000  ...   NaN   NaN   NaN   \n",
       "12    0.684000    0.908000    1.018000    1.086000  ...   NaN   NaN   NaN   \n",
       "13    0.170000    0.224000    0.110000    0.068000  ...   NaN   NaN   NaN   \n",
       "14    9.371808    2.671393    2.912165    3.786173  ...   NaN   NaN   NaN   \n",
       "\n",
       "    1557  1558  1559  1560  1561  1562  1563  \n",
       "0    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "5    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "6    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "7    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "8    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "9    NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "10   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "11   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "12   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "13   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "14   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[15 rows x 1564 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/khokhlovam/Documents/kotelnikov/data/data_lstm_test_complet.csv'\n",
    "df =pd.read_csv(data_path, header=None, names=range(1564)) # \n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072dbe7-7091-402a-8238-11d9f2c4e3ae",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "We need to clean the data (remove NaNs, and take each time only 128 dimensionalities sub-sequences), and then one sample per patient will be:\n",
    "\n",
    "[9, 128]\n",
    "\n",
    "NB: one of the features, time, can be removed, but so far I am leaving everything in for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "383c5607-ac38-48b9-b68e-a98ffe4f2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper script to cut the data into sub-parts:\n",
    "def cut_and_store(df, N, save_folder):\n",
    "    '''\n",
    "    df : dataset with expected structure\n",
    "    N (int): sequence to take length\n",
    "    save_folder: folder to save the dataset\n",
    "    '''\n",
    "    # prepare a PD frame \n",
    "    new_df = pd.DataFrame(columns=['file_name', 'label', 'patient_id'])  \n",
    "    for i in range(0,len(df),9): # go for each patient \n",
    "        # for each patient, make csv files     \n",
    "        label = df.iloc[[i]][0].values[0] \n",
    "        patient_id = df.iloc[[i]][1].values[0] \n",
    "        val_range = df.iloc[[i]].values.tolist()[0]\n",
    "        valid_indexes = [i for i in range(3,len(val_range)) if str(val_range[i]) !='nan']\n",
    "        current_feat =  df.iloc[[i]].values.tolist()[0]\n",
    "        current_feat_clean = [current_feat[x] for x in valid_indexes]\n",
    "        num_wavetrains = len(current_feat_clean)   #update once based on frequency, overwritten next      \n",
    "        for k in range(math.floor(num_wavetrains/N)-1): # for each window\n",
    "            print(f'Patient {patient_id}, subset {k}, label {label}')            \n",
    "            with open(save_folder + f'/{i}_{k}.csv', 'w', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                for j in range(i, i+9): # all features used\n",
    "                   current_feat =  df.iloc[[j]].values.tolist()[0]\n",
    "                   current_feat_clean = [current_feat[x] for x in valid_indexes]\n",
    "                   wr.writerow(current_feat_clean[k:k+N])\n",
    "                   new_df.loc[len(new_df.index)] = [f'{i}_{k}.csv',label, patient_id] \n",
    "    # save for dataloader\n",
    "    new_df.to_csv(save_folder+f'/all_data_{N}.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "afd67955-c37c-4dd7-b0cc-67b6f6bbeb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 4, subset 0, label control\n",
      "Patient 4, subset 1, label control\n",
      "Patient 4, subset 2, label control\n",
      "Patient 4, subset 3, label control\n",
      "Patient 4, subset 4, label control\n",
      "Patient 4, subset 5, label control\n",
      "Patient 4, subset 6, label control\n",
      "Patient 4, subset 7, label control\n",
      "Patient 4, subset 8, label control\n",
      "Patient 6, subset 0, label control\n",
      "Patient 6, subset 1, label control\n",
      "Patient 6, subset 2, label control\n",
      "Patient 6, subset 3, label control\n",
      "Patient 6, subset 4, label control\n",
      "Patient 6, subset 5, label control\n",
      "Patient 6, subset 6, label control\n",
      "Patient 6, subset 7, label control\n",
      "Patient 3, subset 0, label control\n",
      "Patient 3, subset 1, label control\n",
      "Patient 3, subset 2, label control\n",
      "Patient 3, subset 3, label control\n",
      "Patient 3, subset 4, label control\n",
      "Patient 3, subset 5, label control\n",
      "Patient 3, subset 6, label control\n",
      "Patient 3, subset 7, label control\n",
      "Patient 0, subset 0, label control\n",
      "Patient 0, subset 1, label control\n",
      "Patient 0, subset 2, label control\n",
      "Patient 0, subset 3, label control\n",
      "Patient 0, subset 4, label control\n",
      "Patient 0, subset 5, label control\n",
      "Patient 0, subset 6, label control\n",
      "Patient 0, subset 7, label control\n",
      "Patient 2, subset 0, label control\n",
      "Patient 2, subset 1, label control\n",
      "Patient 2, subset 2, label control\n",
      "Patient 2, subset 3, label control\n",
      "Patient 2, subset 4, label control\n",
      "Patient 2, subset 5, label control\n",
      "Patient 2, subset 6, label control\n",
      "Patient 2, subset 7, label control\n",
      "Patient 2, subset 8, label control\n",
      "Patient 7, subset 0, label control\n",
      "Patient 7, subset 1, label control\n",
      "Patient 7, subset 2, label control\n",
      "Patient 7, subset 3, label control\n",
      "Patient 7, subset 4, label control\n",
      "Patient 7, subset 5, label control\n",
      "Patient 7, subset 6, label control\n",
      "Patient 7, subset 7, label control\n",
      "Patient 7, subset 8, label control\n",
      "Patient 1, subset 0, label control\n",
      "Patient 1, subset 1, label control\n",
      "Patient 1, subset 2, label control\n",
      "Patient 1, subset 3, label control\n",
      "Patient 1, subset 4, label control\n",
      "Patient 1, subset 5, label control\n",
      "Patient 1, subset 6, label control\n",
      "Patient 1, subset 7, label control\n",
      "Patient 1, subset 8, label control\n",
      "Patient 5, subset 0, label control\n",
      "Patient 5, subset 1, label control\n",
      "Patient 5, subset 2, label control\n",
      "Patient 5, subset 3, label control\n",
      "Patient 5, subset 4, label control\n",
      "Patient 5, subset 5, label control\n",
      "Patient 5, subset 6, label control\n",
      "Patient 5, subset 7, label control\n",
      "Patient 15, subset 0, label PDL\n",
      "Patient 15, subset 1, label PDL\n",
      "Patient 15, subset 2, label PDL\n",
      "Patient 15, subset 3, label PDL\n",
      "Patient 15, subset 4, label PDL\n",
      "Patient 15, subset 5, label PDL\n",
      "Patient 8, subset 0, label PDL\n",
      "Patient 8, subset 1, label PDL\n",
      "Patient 8, subset 2, label PDL\n",
      "Patient 8, subset 3, label PDL\n",
      "Patient 8, subset 4, label PDL\n",
      "Patient 13, subset 0, label PDL\n",
      "Patient 13, subset 1, label PDL\n",
      "Patient 13, subset 2, label PDL\n",
      "Patient 13, subset 3, label PDL\n",
      "Patient 11, subset 0, label PDL\n",
      "Patient 11, subset 1, label PDL\n",
      "Patient 11, subset 2, label PDL\n",
      "Patient 11, subset 3, label PDL\n",
      "Patient 11, subset 4, label PDL\n",
      "Patient 11, subset 5, label PDL\n",
      "Patient 11, subset 6, label PDL\n",
      "Patient 11, subset 7, label PDL\n",
      "Patient 14, subset 0, label PDL\n",
      "Patient 14, subset 1, label PDL\n",
      "Patient 14, subset 2, label PDL\n",
      "Patient 14, subset 3, label PDL\n",
      "Patient 14, subset 4, label PDL\n",
      "Patient 16, subset 0, label PDL\n",
      "Patient 16, subset 1, label PDL\n",
      "Patient 16, subset 2, label PDL\n",
      "Patient 16, subset 3, label PDL\n",
      "Patient 9, subset 0, label PDL\n",
      "Patient 9, subset 1, label PDL\n",
      "Patient 9, subset 2, label PDL\n",
      "Patient 9, subset 3, label PDL\n",
      "Patient 9, subset 4, label PDL\n",
      "Patient 9, subset 5, label PDL\n",
      "Patient 9, subset 6, label PDL\n",
      "Patient 12, subset 0, label PDL\n",
      "Patient 12, subset 1, label PDL\n",
      "Patient 12, subset 2, label PDL\n",
      "Patient 12, subset 3, label PDL\n",
      "Patient 10, subset 0, label PDL\n",
      "Patient 10, subset 1, label PDL\n",
      "Patient 10, subset 2, label PDL\n",
      "Patient 10, subset 3, label PDL\n",
      "Patient 10, subset 4, label PDL\n",
      "Patient 10, subset 5, label PDL\n",
      "Patient 10, subset 6, label PDL\n",
      "Patient 25, subset 0, label PDR\n",
      "Patient 25, subset 1, label PDR\n",
      "Patient 25, subset 2, label PDR\n",
      "Patient 25, subset 3, label PDR\n",
      "Patient 25, subset 4, label PDR\n",
      "Patient 25, subset 5, label PDR\n",
      "Patient 25, subset 6, label PDR\n",
      "Patient 20, subset 0, label PDR\n",
      "Patient 20, subset 1, label PDR\n",
      "Patient 20, subset 2, label PDR\n",
      "Patient 20, subset 3, label PDR\n",
      "Patient 20, subset 4, label PDR\n",
      "Patient 20, subset 5, label PDR\n",
      "Patient 20, subset 6, label PDR\n",
      "Patient 23, subset 0, label PDR\n",
      "Patient 23, subset 1, label PDR\n",
      "Patient 23, subset 2, label PDR\n",
      "Patient 23, subset 3, label PDR\n",
      "Patient 23, subset 4, label PDR\n",
      "Patient 23, subset 5, label PDR\n",
      "Patient 23, subset 6, label PDR\n",
      "Patient 27, subset 0, label PDR\n",
      "Patient 27, subset 1, label PDR\n",
      "Patient 27, subset 2, label PDR\n",
      "Patient 27, subset 3, label PDR\n",
      "Patient 26, subset 0, label PDR\n",
      "Patient 26, subset 1, label PDR\n",
      "Patient 26, subset 2, label PDR\n",
      "Patient 26, subset 3, label PDR\n",
      "Patient 26, subset 4, label PDR\n",
      "Patient 26, subset 5, label PDR\n",
      "Patient 26, subset 6, label PDR\n",
      "Patient 26, subset 7, label PDR\n",
      "Patient 26, subset 8, label PDR\n",
      "Patient 24, subset 0, label PDR\n",
      "Patient 24, subset 1, label PDR\n",
      "Patient 24, subset 2, label PDR\n",
      "Patient 24, subset 3, label PDR\n",
      "Patient 24, subset 4, label PDR\n",
      "Patient 24, subset 5, label PDR\n",
      "Patient 18, subset 0, label PDR\n",
      "Patient 18, subset 1, label PDR\n",
      "Patient 18, subset 2, label PDR\n",
      "Patient 18, subset 3, label PDR\n",
      "Patient 22, subset 0, label PDR\n",
      "Patient 22, subset 1, label PDR\n",
      "Patient 22, subset 2, label PDR\n",
      "Patient 22, subset 3, label PDR\n",
      "Patient 22, subset 4, label PDR\n",
      "Patient 21, subset 0, label PDR\n",
      "Patient 21, subset 1, label PDR\n",
      "Patient 21, subset 2, label PDR\n",
      "Patient 21, subset 3, label PDR\n",
      "Patient 21, subset 4, label PDR\n",
      "Patient 21, subset 5, label PDR\n",
      "Patient 21, subset 6, label PDR\n",
      "Patient 21, subset 7, label PDR\n",
      "Patient 17, subset 0, label PDR\n",
      "Patient 17, subset 1, label PDR\n",
      "Patient 17, subset 2, label PDR\n",
      "Patient 17, subset 3, label PDR\n",
      "Patient 17, subset 4, label PDR\n",
      "Patient 17, subset 5, label PDR\n",
      "Patient 17, subset 6, label PDR\n",
      "Patient 17, subset 7, label PDR\n",
      "Patient 19, subset 0, label PDR\n",
      "Patient 19, subset 1, label PDR\n",
      "Patient 19, subset 2, label PDR\n",
      "Patient 19, subset 3, label PDR\n",
      "Patient 19, subset 4, label PDR\n",
      "Patient 38, subset 0, label ET\n",
      "Patient 38, subset 1, label ET\n",
      "Patient 38, subset 2, label ET\n",
      "Patient 38, subset 3, label ET\n",
      "Patient 38, subset 4, label ET\n",
      "Patient 38, subset 5, label ET\n",
      "Patient 38, subset 6, label ET\n",
      "Patient 38, subset 7, label ET\n",
      "Patient 38, subset 8, label ET\n",
      "Patient 39, subset 0, label ET\n",
      "Patient 39, subset 1, label ET\n",
      "Patient 39, subset 2, label ET\n",
      "Patient 39, subset 3, label ET\n",
      "Patient 28, subset 0, label ET\n",
      "Patient 28, subset 1, label ET\n",
      "Patient 28, subset 2, label ET\n",
      "Patient 28, subset 3, label ET\n",
      "Patient 28, subset 4, label ET\n",
      "Patient 28, subset 5, label ET\n",
      "Patient 28, subset 6, label ET\n",
      "Patient 28, subset 7, label ET\n",
      "Patient 28, subset 8, label ET\n",
      "Patient 40, subset 0, label ET\n",
      "Patient 40, subset 1, label ET\n",
      "Patient 40, subset 2, label ET\n",
      "Patient 40, subset 3, label ET\n",
      "Patient 40, subset 4, label ET\n",
      "Patient 40, subset 5, label ET\n",
      "Patient 40, subset 6, label ET\n",
      "Patient 40, subset 7, label ET\n",
      "Patient 40, subset 8, label ET\n",
      "Patient 37, subset 0, label ET\n",
      "Patient 37, subset 1, label ET\n",
      "Patient 37, subset 2, label ET\n",
      "Patient 37, subset 3, label ET\n",
      "Patient 37, subset 4, label ET\n",
      "Patient 37, subset 5, label ET\n",
      "Patient 37, subset 6, label ET\n",
      "Patient 37, subset 7, label ET\n",
      "Patient 37, subset 8, label ET\n",
      "Patient 36, subset 0, label ET\n",
      "Patient 36, subset 1, label ET\n",
      "Patient 36, subset 2, label ET\n",
      "Patient 36, subset 3, label ET\n",
      "Patient 36, subset 4, label ET\n",
      "Patient 36, subset 5, label ET\n",
      "Patient 36, subset 6, label ET\n",
      "Patient 36, subset 7, label ET\n",
      "Patient 33, subset 0, label ET\n",
      "Patient 33, subset 1, label ET\n",
      "Patient 33, subset 2, label ET\n",
      "Patient 33, subset 3, label ET\n",
      "Patient 33, subset 4, label ET\n",
      "Patient 33, subset 5, label ET\n",
      "Patient 33, subset 6, label ET\n",
      "Patient 33, subset 7, label ET\n",
      "Patient 35, subset 0, label ET\n",
      "Patient 35, subset 1, label ET\n",
      "Patient 35, subset 2, label ET\n",
      "Patient 35, subset 3, label ET\n",
      "Patient 35, subset 4, label ET\n",
      "Patient 35, subset 5, label ET\n",
      "Patient 35, subset 6, label ET\n",
      "Patient 35, subset 7, label ET\n",
      "Patient 30, subset 0, label ET\n",
      "Patient 30, subset 1, label ET\n",
      "Patient 30, subset 2, label ET\n",
      "Patient 30, subset 3, label ET\n",
      "Patient 30, subset 4, label ET\n",
      "Patient 30, subset 5, label ET\n",
      "Patient 30, subset 6, label ET\n",
      "Patient 30, subset 7, label ET\n",
      "Patient 30, subset 8, label ET\n",
      "Patient 34, subset 0, label ET\n",
      "Patient 34, subset 1, label ET\n",
      "Patient 34, subset 2, label ET\n",
      "Patient 34, subset 3, label ET\n",
      "Patient 34, subset 4, label ET\n",
      "Patient 34, subset 5, label ET\n",
      "Patient 34, subset 6, label ET\n",
      "Patient 34, subset 7, label ET\n",
      "Patient 31, subset 0, label ET\n",
      "Patient 31, subset 1, label ET\n",
      "Patient 31, subset 2, label ET\n",
      "Patient 31, subset 3, label ET\n",
      "Patient 31, subset 4, label ET\n",
      "Patient 31, subset 5, label ET\n",
      "Patient 31, subset 6, label ET\n",
      "Patient 31, subset 7, label ET\n",
      "Patient 32, subset 0, label ET\n",
      "Patient 32, subset 1, label ET\n",
      "Patient 32, subset 2, label ET\n",
      "Patient 32, subset 3, label ET\n",
      "Patient 32, subset 4, label ET\n",
      "Patient 32, subset 5, label ET\n",
      "Patient 32, subset 6, label ET\n",
      "Patient 32, subset 7, label ET\n",
      "Patient 29, subset 0, label ET\n",
      "Patient 29, subset 1, label ET\n",
      "Patient 29, subset 2, label ET\n",
      "Patient 29, subset 3, label ET\n",
      "Patient 29, subset 4, label ET\n",
      "Patient 29, subset 5, label ET\n",
      "Patient 29, subset 6, label ET\n",
      "Patient 29, subset 7, label ET\n"
     ]
    }
   ],
   "source": [
    "cut_and_store(df, 128, 'C:/Users/khokhlovam/Documents/kotelnikov/data/lstm_trials/n128_9_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "505e42d4-5b70-4d47-9dc4-e41440ced97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PDControlDataset(Dataset):\n",
    "    \"\"\"Neurogenertive features dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file) # \n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        file_path = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 1])\n",
    "        label = self.convert_label(self.df.iloc[idx, 2])\n",
    "        features  = pd.read_csv(file_path,  header=None).values\n",
    "        sample = {'data': features, 'label': label}\n",
    "\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def convert_label(self, label):\n",
    "        if label == 'control':\n",
    "            return 0\n",
    "        elif label == 'PDL':\n",
    "            return 1\n",
    "        elif label == 'PDR':\n",
    "            return 1\n",
    "        elif label == 'ET':\n",
    "            return 2    \n",
    "        else:\n",
    "            raise Exception('ONLY control, PDL, PDR en ET are currently supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2be3cc42-96b0-4ef8-b8ef-cf2fc9329c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_p = 'C:/Users/khokhlovam/Documents/kotelnikov/data/lstm_trials/n128_9_features/'              \n",
    "dataset = PDControlDataset(folder_p +'all_data_128.csv', root_dir=folder_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97adee4c-d715-4677-8e36-13d1d2943656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [ 612 1080  945]\n"
     ]
    }
   ],
   "source": [
    "# check data statistics\n",
    "labels = [0]*len(dataset)\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    labels[i] = sample['label']\n",
    "values, counts = np.unique(labels, return_counts=True)\n",
    "print(values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b71e7f06-b221-4345-bc54-6af531591362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 128)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    labels[i] = sample['label']\n",
    "    data = sample['data']\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c61edb5a-3ffc-4c57-8108-0b6ef1bfe78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1,shuffle=True)  \n",
    "for sample in dataloader:\n",
    "    print(sample['data'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c1f3d-2fe4-413f-8975-d06d10bbbaf3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64277a-3e27-401a-852e-cff9275f64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35] - Loss: 1.1933 - Accuracy: 0.3587\n",
      "Epoch [2/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [3/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [4/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [5/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [6/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [7/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [8/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [9/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [10/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [11/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [12/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [13/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [14/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [15/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [16/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [17/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [18/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [19/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [20/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [21/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [22/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [23/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [24/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [25/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [26/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [27/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [28/35] - Loss: 1.1935 - Accuracy: 0.3584\n",
      "Epoch [29/35] - Loss: 1.1924 - Accuracy: 0.3584\n",
      "Epoch [30/35] - Loss: 1.1924 - Accuracy: 0.3584\n"
     ]
    }
   ],
   "source": [
    "n_features = 9 #\n",
    "n_classes = 3\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet1D(n_features = 9, n_classes = 3, n_timesteps = 128)\n",
    "# opt and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "# Number of training epochs\n",
    "num_epochs = 35\n",
    "# data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,shuffle=True)  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_predictions = []\n",
    "    total_labels = []\n",
    "    for batch in dataloader:  # Use your data loader\n",
    "        batch_data = batch['data'].float().to(device)\n",
    "        batch_labels = batch['label'].long().to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_predictions += outputs.argmax(dim=1).cpu().numpy().tolist()\n",
    "        total_labels += batch_labels.cpu().numpy().tolist()\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(total_labels, total_predictions)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "# After training, you can save the model\n",
    "torch.save(model.state_dict(), '35_epoch.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda97b6-0db7-4142-8f81-538b0fc919ca",
   "metadata": {},
   "source": [
    "## Verification of the model based on Keras model from the author\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_6 (Conv1D)            (None, 126, 64)           1792      \n",
    "_________________________________________________________________\n",
    "conv1d_7 (Conv1D)            (None, 124, 64)           12352     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 124, 64)           0         \n",
    "_________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1 (None, 62, 64)            0         \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 3968)              0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 100)               396900    \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 411,246\n",
    "Trainable params: 411,246\n",
    "Non-trainable params: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23db685c-927f-440b-aebf-4c75773b2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3968, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# torch.Size([64, 64, 126])\n",
    "# torch.Size([64, 64, 62])\n",
    "# torch.Size([64, 3968])\n",
    "# torch.Size([64, 100])\n",
    "# torch.Size([64, 2])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785dc50-065d-4e61-927c-308d47772ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate attention weights (assuming x is the output from the previous layer)\n",
    "        attention_weights = torch.softmax(x, dim=1)\n",
    "        # Apply attention to the input\n",
    "        attended_input = torch.sum(x * attention_weights, dim=1)\n",
    "        return attended_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4acd4-444e-4b49-850e-7838c551dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, n_timesteps, n_features, n_outputs):\n",
    "        super(CustomModel, self).__init()\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_features, out_channels=64, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        input_size = 64 * ((n_timesteps - 4) // 2)\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.attention = AttentionLayer()  # Add the attention layer\n",
    "        self.fc2 = nn.Linear(100, n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.attention(x)  # Apply attention\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Example input shape\n",
    "n_timesteps = 128\n",
    "n_features = 1\n",
    "n_outputs = 10\n",
    "\n",
    "model = CustomModel(n_timesteps, n_features, n_outputs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# In this example, an AttentionLayer is added before the last fully connected layer (fc2). The attention mechanism applies attention weights to the output of the previous layer and calculates a weighted sum, which is then passed to the last layer. You can customize the attention mechanism further based on your specific requirements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
