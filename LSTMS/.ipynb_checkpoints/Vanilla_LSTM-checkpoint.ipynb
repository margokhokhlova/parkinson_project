{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fb6f73",
   "metadata": {},
   "source": [
    "Inspired by this Vanilla [model](https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312bf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3ab19",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load the features we previosly prepared for our patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea27ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/KHOKHLOVAM/Documents/projects/kotelnikov/data_lstm_test1.csv'\n",
    "df =pd.read_csv(data_path, header=None, names=range(1564)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36cb826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1554</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "      <th>1558</th>\n",
       "      <th>1559</th>\n",
       "      <th>1560</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1563</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>control</td>\n",
       "      <td>0</td>\n",
       "      <td>23.7000</td>\n",
       "      <td>13.1000</td>\n",
       "      <td>38.400</td>\n",
       "      <td>9.2000</td>\n",
       "      <td>25.4000</td>\n",
       "      <td>30.4000</td>\n",
       "      <td>38.1000</td>\n",
       "      <td>13.4000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>control</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0902</td>\n",
       "      <td>1.1266</td>\n",
       "      <td>1.152</td>\n",
       "      <td>2.4472</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>1.1552</td>\n",
       "      <td>1.2954</td>\n",
       "      <td>1.0184</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>0</td>\n",
       "      <td>8.8000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>12.800</td>\n",
       "      <td>3.1000</td>\n",
       "      <td>13.2000</td>\n",
       "      <td>11.3000</td>\n",
       "      <td>14.1000</td>\n",
       "      <td>6.2000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>1</td>\n",
       "      <td>39.2000</td>\n",
       "      <td>40.5000</td>\n",
       "      <td>14.800</td>\n",
       "      <td>36.8000</td>\n",
       "      <td>24.6000</td>\n",
       "      <td>5.6000</td>\n",
       "      <td>29.5000</td>\n",
       "      <td>28.1000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1        2        3       4        5        6        7     \\\n",
       "0  control     0  23.7000  13.1000  38.400   9.2000  25.4000  30.4000   \n",
       "1  control     0   1.0902   1.1266   1.152   2.4472   0.9652   1.1552   \n",
       "2  control     0   8.8000   4.0000  12.800   3.1000  13.2000  11.3000   \n",
       "3  control     0   0.1480   0.0820   0.096   0.0820   0.0640   0.0820   \n",
       "4  control     1  39.2000  40.5000  14.800  36.8000  24.6000   5.6000   \n",
       "\n",
       "      8        9     ...  1554  1555  1556  1557  1558  1559  1560  1561  \\\n",
       "0  38.1000  13.4000  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   1.2954   1.0184  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2  14.1000   6.2000  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3   0.1280   0.1700  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4  29.5000  28.1000  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   1562  1563  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  \n",
       "\n",
       "[5 rows x 1564 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "# todo - add a column with name of the feature. Feature order is: \n",
    "# freq\n",
    "# flash_d\n",
    "# bandw\n",
    "# time_lstm (time since last wavetrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae95a1",
   "metadata": {},
   "source": [
    "First simple idea on how to use the our data:\n",
    "1) Take a fixed length sub-sequences from data, and not very very long. Do not use any embedding, but use initial 4 features insted of an embedding, then the input into the LSTM will be:\n",
    "*batch_size,seq_length (N),embedding_dimension(4)*\n",
    "**NB:** *embedding dimension is composed of: freq, flash_d, bandw,time_lstm (time since last wavetrain)*\n",
    "\n",
    "2) Do not care about the separation of the patients for train and validation set for the moment.\n",
    "3) Use only PD and Control for the moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "529d223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper script to cut the data into sub-parts:\n",
    "def cut_and_store(csv_file, N, save_folder, max_len=1564):\n",
    "    '''\n",
    "    N (int): sequence to take length\n",
    "    max_len: init max lenght of the wavetrain full sequence\n",
    "    '''\n",
    "    df = pd.read_csv(data_path, header=None, names=range(max_len)) # \n",
    "    # drop ET data\n",
    "    df = df[df[0] != 'ET']\n",
    "    # prepare a PD frame \n",
    "    new_df = pd.DataFrame(columns=['file_name', 'label', 'patient_id'])  \n",
    "    for i in range(0,len(df),4):\n",
    "        # for each patient, make csv files        \n",
    "        single_row_fr = df.iloc[[i]].values.flatten().tolist()\n",
    "        patient_id = single_row_fr[1]\n",
    "        patient_label = single_row_fr[0]\n",
    "        cleaned_freq = [x for x in single_row_fr[2:] if str(x) != 'nan']\n",
    "        single_flash_d = df.iloc[[i+1]].values.flatten().tolist()[2:]\n",
    "        cleanded_flash_d = [x for x in single_flash_d  if str(x) != 'nan']\n",
    "        single_bandw = df.iloc[[i+2]].values.flatten().tolist()[2:]\n",
    "        cleaned_bandw = [x for x in single_bandw  if str(x) != 'nan']\n",
    "        single_time = df.iloc[[i+3]].values.flatten().tolist()[2:]\n",
    "        cleaned_time_lstm = [x for x in   single_time if str(x) != 'nan']\n",
    "        num_wavetrains = len(cleaned_freq)        \n",
    "        for j in range(math.floor(num_wavetrains/N)-1):\n",
    "            print(f'Patient {i}, subset {j}, label {patient_label}')\n",
    "            with open(save_folder + f'/{i}_{j}.csv', 'w', newline='') as myfile:\n",
    "                wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "                wr.writerow(cleaned_freq[j:j+N])\n",
    "                wr.writerow(cleanded_flash_d[j:j+N])\n",
    "                wr.writerow(cleaned_bandw[j:j+N])\n",
    "                wr.writerow(cleaned_time_lstm[j:j+N])\n",
    "                new_df.loc[len(new_df.index)] = [f'{i}_{j}.csv',patient_label, patient_id] \n",
    "        # save for dataloader\n",
    "    new_df.to_csv(save_folder+'/all_data_200N.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a2678f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 0, subset 0, label control\n",
      "Patient 0, subset 1, label control\n",
      "Patient 0, subset 2, label control\n",
      "Patient 0, subset 3, label control\n",
      "Patient 4, subset 0, label control\n",
      "Patient 4, subset 1, label control\n",
      "Patient 4, subset 2, label control\n",
      "Patient 4, subset 3, label control\n",
      "Patient 4, subset 4, label control\n",
      "Patient 4, subset 5, label control\n",
      "Patient 8, subset 0, label control\n",
      "Patient 8, subset 1, label control\n",
      "Patient 8, subset 2, label control\n",
      "Patient 8, subset 3, label control\n",
      "Patient 12, subset 0, label control\n",
      "Patient 12, subset 1, label control\n",
      "Patient 12, subset 2, label control\n",
      "Patient 12, subset 3, label control\n",
      "Patient 12, subset 4, label control\n",
      "Patient 16, subset 0, label control\n",
      "Patient 16, subset 1, label control\n",
      "Patient 16, subset 2, label control\n",
      "Patient 16, subset 3, label control\n",
      "Patient 16, subset 4, label control\n",
      "Patient 20, subset 0, label control\n",
      "Patient 20, subset 1, label control\n",
      "Patient 20, subset 2, label control\n",
      "Patient 20, subset 3, label control\n",
      "Patient 20, subset 4, label control\n",
      "Patient 24, subset 0, label control\n",
      "Patient 24, subset 1, label control\n",
      "Patient 24, subset 2, label control\n",
      "Patient 24, subset 3, label control\n",
      "Patient 24, subset 4, label control\n",
      "Patient 28, subset 0, label control\n",
      "Patient 28, subset 1, label control\n",
      "Patient 28, subset 2, label control\n",
      "Patient 28, subset 3, label control\n",
      "Patient 28, subset 4, label control\n",
      "Patient 32, subset 0, label PDL\n",
      "Patient 32, subset 1, label PDL\n",
      "Patient 32, subset 2, label PDL\n",
      "Patient 32, subset 3, label PDL\n",
      "Patient 32, subset 4, label PDL\n",
      "Patient 36, subset 0, label PDL\n",
      "Patient 36, subset 1, label PDL\n",
      "Patient 36, subset 2, label PDL\n",
      "Patient 36, subset 3, label PDL\n",
      "Patient 36, subset 4, label PDL\n",
      "Patient 40, subset 0, label PDL\n",
      "Patient 40, subset 1, label PDL\n",
      "Patient 40, subset 2, label PDL\n",
      "Patient 40, subset 3, label PDL\n",
      "Patient 40, subset 4, label PDL\n",
      "Patient 44, subset 0, label PDL\n",
      "Patient 44, subset 1, label PDL\n",
      "Patient 44, subset 2, label PDL\n",
      "Patient 44, subset 3, label PDL\n",
      "Patient 48, subset 0, label PDL\n",
      "Patient 48, subset 1, label PDL\n",
      "Patient 48, subset 2, label PDL\n",
      "Patient 48, subset 3, label PDL\n",
      "Patient 48, subset 4, label PDL\n",
      "Patient 52, subset 0, label PDL\n",
      "Patient 52, subset 1, label PDL\n",
      "Patient 52, subset 2, label PDL\n",
      "Patient 52, subset 3, label PDL\n",
      "Patient 52, subset 4, label PDL\n",
      "Patient 56, subset 0, label PDL\n",
      "Patient 56, subset 1, label PDL\n",
      "Patient 56, subset 2, label PDL\n",
      "Patient 56, subset 3, label PDL\n",
      "Patient 60, subset 0, label PDL\n",
      "Patient 60, subset 1, label PDL\n",
      "Patient 60, subset 2, label PDL\n",
      "Patient 60, subset 3, label PDL\n",
      "Patient 60, subset 4, label PDL\n",
      "Patient 64, subset 0, label PDL\n",
      "Patient 64, subset 1, label PDL\n",
      "Patient 64, subset 2, label PDL\n",
      "Patient 64, subset 3, label PDL\n",
      "Patient 64, subset 4, label PDL\n",
      "Patient 68, subset 0, label PDR\n",
      "Patient 68, subset 1, label PDR\n",
      "Patient 72, subset 0, label PDR\n",
      "Patient 72, subset 1, label PDR\n",
      "Patient 72, subset 2, label PDR\n",
      "Patient 72, subset 3, label PDR\n",
      "Patient 72, subset 4, label PDR\n",
      "Patient 76, subset 0, label PDR\n",
      "Patient 76, subset 1, label PDR\n",
      "Patient 76, subset 2, label PDR\n",
      "Patient 76, subset 3, label PDR\n",
      "Patient 76, subset 4, label PDR\n",
      "Patient 80, subset 0, label PDR\n",
      "Patient 80, subset 1, label PDR\n",
      "Patient 80, subset 2, label PDR\n",
      "Patient 80, subset 3, label PDR\n",
      "Patient 80, subset 4, label PDR\n",
      "Patient 84, subset 0, label PDR\n",
      "Patient 84, subset 1, label PDR\n",
      "Patient 84, subset 2, label PDR\n",
      "Patient 84, subset 3, label PDR\n",
      "Patient 84, subset 4, label PDR\n",
      "Patient 88, subset 0, label PDR\n",
      "Patient 88, subset 1, label PDR\n",
      "Patient 88, subset 2, label PDR\n",
      "Patient 88, subset 3, label PDR\n",
      "Patient 88, subset 4, label PDR\n",
      "Patient 92, subset 0, label PDR\n",
      "Patient 92, subset 1, label PDR\n",
      "Patient 92, subset 2, label PDR\n",
      "Patient 92, subset 3, label PDR\n",
      "Patient 92, subset 4, label PDR\n",
      "Patient 96, subset 0, label PDR\n",
      "Patient 96, subset 1, label PDR\n",
      "Patient 96, subset 2, label PDR\n",
      "Patient 96, subset 3, label PDR\n",
      "Patient 96, subset 4, label PDR\n",
      "Patient 100, subset 0, label PDR\n",
      "Patient 100, subset 1, label PDR\n",
      "Patient 100, subset 2, label PDR\n",
      "Patient 100, subset 3, label PDR\n",
      "Patient 100, subset 4, label PDR\n",
      "Patient 104, subset 0, label PDR\n",
      "Patient 104, subset 1, label PDR\n",
      "Patient 104, subset 2, label PDR\n",
      "Patient 104, subset 3, label PDR\n",
      "Patient 104, subset 4, label PDR\n",
      "Patient 108, subset 0, label PDR\n",
      "Patient 108, subset 1, label PDR\n",
      "Patient 108, subset 2, label PDR\n",
      "Patient 108, subset 3, label PDR\n",
      "Patient 108, subset 4, label PDR\n",
      "Patient 112, subset 0, label PDR\n",
      "Patient 112, subset 1, label PDR\n",
      "Patient 112, subset 2, label PDR\n",
      "Patient 112, subset 3, label PDR\n",
      "Patient 116, subset 0, label PDR\n",
      "Patient 116, subset 1, label PDR\n",
      "Patient 116, subset 2, label PDR\n",
      "Patient 116, subset 3, label PDR\n",
      "Patient 116, subset 4, label PDR\n"
     ]
    }
   ],
   "source": [
    "# Actual cut, just specify N and the folder to save the dataset\n",
    "folder_p = 'C:/Users/KHOKHLOVAM/Documents/projects/kotelnikov/data/lstm_trials/n200_4_features/'              \n",
    "cut_and_store('C:/Users/KHOKHLOVAM/Documents/projects/kotelnikov/data_lstm_test1.csv', 200, folder_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5992de09",
   "metadata": {},
   "source": [
    "### Create first training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PDControlDataset(Dataset):\n",
    "    \"\"\"Neurogenertive features dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file) # \n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        file_path = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 1])\n",
    "        label = self.convert_label(self.df.iloc[idx, 2])\n",
    "        features  = pd.read_csv(file_path,  header=None).values\n",
    "        sample = {'data': features, 'label': label}\n",
    "\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def convert_label(self, label):\n",
    "        if label == 'control':\n",
    "            return 0\n",
    "        elif label == 'PDL':\n",
    "            return 1\n",
    "        elif label == 'PDR':\n",
    "            return 1\n",
    "        else:\n",
    "            raise Exception('ONLY control, PDL, PDR are currently supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530e25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_p = 'C:/Users/KHOKHLOVAM/Documents/projects/kotelnikov/data/lstm_trials/n200_4_features/'              \n",
    "dataset = PDControlDataset(folder_p +'/all_data_200N.csv', root_dir=folder_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf5082",
   "metadata": {},
   "source": [
    "The dataset is very unbalanced... There can be two reasons:\n",
    "There are more Parkinson patients than the control patients.\n",
    "There are more wavetrains in Parkinson patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39db491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [ 39 104]\n"
     ]
    }
   ],
   "source": [
    "# check data statistics\n",
    "labels = [0]*len(dataset)\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    labels[i] = sample['label']\n",
    "values, counts = np.unique(labels, return_counts=True)\n",
    "print(values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11da1f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'data': array([[2.3700e+01, 1.3100e+01, 3.8400e+01, 9.2000e+00, 2.5400e+01,\n",
      "        3.0400e+01, 3.8100e+01, 1.3400e+01, 3.5200e+01, 3.4800e+01,\n",
      "        2.1800e+01, 2.5400e+01, 9.9000e+00, 2.6600e+01, 1.3400e+01,\n",
      "        2.6000e+01, 4.1000e+00, 3.0900e+01, 1.2800e+01, 2.5600e+01,\n",
      "        2.8200e+01, 1.0600e+01, 4.0500e+01, 1.9600e+01, 3.6800e+01,\n",
      "        8.8000e+00, 2.5600e+01, 4.5000e+00, 2.1300e+01, 3.4700e+01,\n",
      "        2.5900e+01, 9.1000e+00, 1.4900e+01, 3.7300e+01, 1.0800e+01,\n",
      "        1.0000e+00, 2.8000e+01, 3.7200e+01, 1.9200e+01, 3.5200e+01,\n",
      "        2.2100e+01, 1.0600e+01, 2.5000e+00, 3.3700e+01, 3.7300e+01,\n",
      "        2.0500e+01, 1.0300e+01, 3.6800e+01, 1.7800e+01, 2.9500e+01,\n",
      "        2.1300e+01, 2.0200e+01, 7.0000e+00, 1.4100e+01, 2.6000e+00,\n",
      "        3.5000e+01, 2.9500e+01, 1.0900e+01, 6.9000e+00, 3.0900e+01,\n",
      "        3.2900e+01, 1.0900e+01, 2.7700e+01, 3.0800e+01, 1.0600e+01,\n",
      "        1.0600e+01, 3.0900e+01, 9.1000e+00, 2.3400e+01, 2.9500e+01,\n",
      "        9.9000e+00, 3.8100e+01, 8.0000e-01, 1.2300e+01, 3.2800e+01,\n",
      "        2.3000e+00, 2.8300e+01, 9.9000e+00, 2.6400e+01, 2.2200e+01,\n",
      "        2.3100e+01, 7.7000e+00, 1.8000e+00, 2.8400e+01, 2.4600e+01,\n",
      "        8.8000e+00, 3.5000e+01, 2.3000e+01, 3.5000e+01, 6.0000e+00,\n",
      "        2.5500e+01, 1.1600e+01, 3.9200e+01, 7.0000e-01, 7.9000e+00,\n",
      "        2.9000e+00, 1.4100e+01, 2.9800e+01, 1.5500e+01, 3.1900e+01,\n",
      "        9.2000e+00, 4.0400e+01, 1.1400e+01, 4.9000e+00, 1.6700e+01,\n",
      "        2.5100e+01, 1.4200e+01, 3.6800e+01, 4.1600e+01, 3.0900e+01,\n",
      "        3.2000e+00, 3.9900e+01, 7.7000e+00, 1.6400e+01, 9.0000e-01,\n",
      "        2.8000e+01, 3.6800e+01, 1.0200e+01, 3.5000e+01, 9.3000e+00,\n",
      "        1.3700e+01, 4.9000e+00, 3.6800e+01, 1.3700e+01, 3.3000e+01,\n",
      "        1.4900e+01, 2.2700e+01, 7.0000e-01, 1.3400e+01, 3.2500e+01,\n",
      "        1.3600e+01, 1.4000e+00, 2.7700e+01, 6.7000e+00, 2.9600e+01,\n",
      "        3.1000e+00, 3.7300e+01, 1.8400e+01, 1.2800e+01, 9.0000e+00,\n",
      "        1.0300e+01, 1.3600e+01, 2.9500e+01, 7.8000e+00, 4.6000e+00,\n",
      "        3.2800e+01, 1.4900e+01, 2.3200e+01, 1.7000e+00, 2.1100e+01,\n",
      "        8.4000e+00, 3.6000e+00, 3.3500e+01, 1.3600e+01, 1.8700e+01,\n",
      "        7.0000e-01, 5.1000e+00, 1.3800e+01, 3.9300e+01, 2.6600e+01,\n",
      "        2.5600e+01, 2.9000e+00, 3.7900e+01, 3.5800e+01, 3.7100e+01,\n",
      "        3.0500e+01, 3.5900e+01, 1.9500e+01, 4.6000e+00, 1.5000e+00,\n",
      "        3.3800e+01, 2.5600e+01, 1.2800e+01, 2.3400e+01, 5.4000e+00,\n",
      "        1.2600e+01, 2.5000e+00, 3.7300e+01, 1.1000e+01, 2.6600e+01,\n",
      "        2.1300e+01, 3.0700e+01, 1.8600e+01, 8.4000e+00, 1.3400e+01,\n",
      "        3.2400e+01, 3.3600e+01, 2.6600e+01, 8.5000e+00, 2.4600e+01,\n",
      "        1.8700e+01, 1.0600e+01, 3.2000e+00, 2.6100e+01, 8.5000e+00,\n",
      "        1.4100e+01, 3.0700e+01, 4.2100e+01, 1.0200e+01, 1.4300e+01],\n",
      "       [1.0902e+00, 1.1266e+00, 1.1520e+00, 2.4472e+00, 9.6520e-01,\n",
      "        1.1552e+00, 1.2954e+00, 1.0184e+00, 1.1968e+00, 9.7440e-01,\n",
      "        8.2840e-01, 1.1684e+00, 1.4652e+00, 1.3832e+00, 1.2060e+00,\n",
      "        1.1440e+00, 1.0988e+00, 9.8880e-01, 1.5360e+00, 1.8944e+00,\n",
      "        1.0152e+00, 1.1024e+00, 1.2150e+00, 2.0384e+00, 1.0304e+00,\n",
      "        1.1968e+00, 1.1264e+00, 1.2510e+00, 1.1076e+00, 1.3880e+00,\n",
      "        1.3468e+00, 2.2568e+00, 1.4304e+00, 9.6980e-01, 1.4256e+00,\n",
      "        1.4260e+00, 1.0640e+00, 1.0416e+00, 1.3056e+00, 1.0560e+00,\n",
      "        9.7240e-01, 1.0176e+00, 1.6000e+00, 1.0110e+00, 1.1190e+00,\n",
      "        1.1890e+00, 1.2154e+00, 1.1040e+00, 1.1036e+00, 1.2390e+00,\n",
      "        1.7040e+00, 1.3736e+00, 1.1480e+00, 1.0716e+00, 1.2220e+00,\n",
      "        2.0300e+00, 9.4400e-01, 1.6786e+00, 1.2972e+00, 1.1742e+00,\n",
      "        1.0528e+00, 1.2426e+00, 8.8640e-01, 1.1088e+00, 1.2296e+00,\n",
      "        2.6924e+00, 1.3596e+00, 1.5652e+00, 1.0764e+00, 8.2600e-01,\n",
      "        1.3266e+00, 1.0668e+00, 1.1776e+00, 1.1070e+00, 1.0496e+00,\n",
      "        1.0442e+00, 1.3018e+00, 1.4850e+00, 8.9760e-01, 1.0656e+00,\n",
      "        1.2012e+00, 1.1088e+00, 1.2600e+00, 1.1928e+00, 1.1316e+00,\n",
      "        1.0736e+00, 1.0500e+00, 1.1040e+00, 1.1200e+00, 1.1400e+00,\n",
      "        2.7030e+00, 1.4616e+00, 1.0976e+00, 1.4392e+00, 1.5642e+00,\n",
      "        1.2702e+00, 1.7202e+00, 2.4436e+00, 1.5810e+00, 1.2760e+00,\n",
      "        1.1040e+00, 1.3736e+00, 1.2540e+00, 1.1956e+00, 1.2024e+00,\n",
      "        9.5380e-01, 9.9400e-01, 8.8320e-01, 1.9136e+00, 1.0506e+00,\n",
      "        1.6064e+00, 1.3566e+00, 1.3090e+00, 1.3776e+00, 1.3230e+00,\n",
      "        8.9600e-01, 1.0304e+00, 1.1832e+00, 9.1000e-01, 1.2462e+00,\n",
      "        1.0960e+00, 1.2054e+00, 1.1776e+00, 1.0686e+00, 1.8480e+00,\n",
      "        9.8340e-01, 2.3608e+00, 1.6170e+00, 1.2596e+00, 1.0400e+00,\n",
      "        1.0880e+00, 1.5624e+00, 1.6620e+00, 1.3668e+00, 1.0064e+00,\n",
      "        1.5314e+00, 1.1936e+00, 1.2512e+00, 9.2160e-01, 2.7180e+00,\n",
      "        1.5450e+00, 1.2512e+00, 8.8500e-01, 1.3104e+00, 1.3800e+00,\n",
      "        1.3776e+00, 1.0430e+00, 3.1088e+00, 1.5300e+00, 2.2788e+00,\n",
      "        1.0080e+00, 9.6480e-01, 1.2060e+00, 1.3056e+00, 1.5708e+00,\n",
      "        1.0598e+00, 1.0812e+00, 1.1316e+00, 1.0218e+00, 1.2236e+00,\n",
      "        1.1264e+00, 1.3398e+00, 1.0612e+00, 1.2172e+00, 1.1130e+00,\n",
      "        1.0370e+00, 1.1488e+00, 8.9700e-01, 9.5680e-01, 2.5230e+00,\n",
      "        1.0140e+00, 1.0240e+00, 1.1520e+00, 9.3600e-01, 1.1124e+00,\n",
      "        1.3104e+00, 2.4300e+00, 1.0444e+00, 9.0200e-01, 2.2344e+00,\n",
      "        1.6614e+00, 9.2100e-01, 1.4136e+00, 1.1928e+00, 9.6480e-01,\n",
      "        1.1664e+00, 1.4784e+00, 1.0640e+00, 1.6830e+00, 1.0332e+00,\n",
      "        1.4212e+00, 1.4204e+00, 1.3760e+00, 1.6704e+00, 1.1560e+00,\n",
      "        1.0998e+00, 2.1490e+00, 1.2630e+00, 1.1424e+00, 9.4380e-01],\n",
      "       [8.8000e+00, 4.0000e+00, 1.2800e+01, 3.1000e+00, 1.3200e+01,\n",
      "        1.1300e+01, 1.4100e+01, 6.2000e+00, 1.2200e+01, 1.5600e+01,\n",
      "        1.8700e+01, 1.0500e+01, 3.2000e+00, 7.5000e+00, 4.9000e+00,\n",
      "        1.0900e+01, 1.8000e+00, 2.4100e+01, 3.9000e+00, 7.7000e+00,\n",
      "        1.0800e+01, 4.7000e+00, 1.4000e+01, 6.3000e+00, 1.5800e+01,\n",
      "        2.9000e+00, 9.9000e+00, 1.4000e+00, 7.3000e+00, 1.0700e+01,\n",
      "        8.7000e+00, 2.4000e+00, 5.6000e+00, 1.5700e+01, 2.9000e+00,\n",
      "        0.0000e+00, 1.0300e+01, 1.3400e+01, 6.4000e+00, 1.0600e+01,\n",
      "        1.0600e+01, 4.4000e+00, 6.0000e-01, 1.4000e+01, 1.5900e+01,\n",
      "        7.0000e+00, 3.4000e+00, 1.4200e+01, 6.4000e+00, 1.0100e+01,\n",
      "        8.1000e+00, 1.0900e+01, 2.4000e+00, 6.0000e+00, 7.0000e-01,\n",
      "        9.7000e+00, 1.1700e+01, 3.1000e+00, 2.0000e+00, 1.0600e+01,\n",
      "        1.2700e+01, 4.5000e+00, 1.5100e+01, 1.2100e+01, 3.8000e+00,\n",
      "        3.6000e+00, 1.2800e+01, 2.7000e+00, 8.5000e+00, 2.4000e+01,\n",
      "        4.8000e+00, 1.2400e+01, 1.0000e-01, 6.0000e+00, 1.5400e+01,\n",
      "        1.0000e+00, 9.0000e+00, 3.3000e+00, 1.5100e+01, 8.8000e+00,\n",
      "        9.3000e+00, 3.0000e+00, 6.0000e-01, 9.2000e+00, 9.7000e+00,\n",
      "        4.2000e+00, 1.5000e+01, 8.0000e+00, 1.4600e+01, 2.4000e+00,\n",
      "        8.1000e+00, 3.1000e+00, 1.3100e+01, 0.0000e+00, 2.0000e+00,\n",
      "        1.2000e+00, 3.8000e+00, 1.0400e+01, 4.9000e+00, 1.2000e+01,\n",
      "        3.4000e+00, 1.3800e+01, 3.7000e+00, 1.7000e+00, 5.5000e+00,\n",
      "        1.9600e+01, 5.8000e+00, 1.9300e+01, 1.0900e+01, 1.3600e+01,\n",
      "        1.2000e+00, 1.4200e+01, 2.8000e+00, 1.4500e+01, 2.0000e-01,\n",
      "        1.1600e+01, 1.2200e+01, 3.9000e+00, 1.5500e+01, 3.3000e+00,\n",
      "        1.9000e+01, 1.4000e+00, 1.3000e+01, 5.5000e+00, 9.4000e+00,\n",
      "        6.2000e+00, 8.6000e+00, 1.0000e-01, 5.3000e+00, 1.1300e+01,\n",
      "        5.8000e+00, 6.0000e-01, 8.6000e+00, 2.7000e+00, 1.3000e+01,\n",
      "        9.0000e-01, 1.0200e+01, 6.3000e+00, 7.1000e+00, 2.6000e+00,\n",
      "        5.3000e+00, 5.7000e+00, 1.5500e+01, 2.8000e+00, 1.2000e+00,\n",
      "        1.1400e+01, 6.0000e+00, 6.2000e+00, 1.0000e-01, 6.6000e+00,\n",
      "        3.2000e+00, 1.9000e+00, 1.1400e+01, 4.8000e+00, 5.9000e+00,\n",
      "        0.0000e+00, 1.8000e+00, 5.4000e+00, 1.5500e+01, 8.6000e+00,\n",
      "        1.0800e+01, 8.0000e-01, 1.5100e+01, 1.3100e+01, 1.6200e+01,\n",
      "        1.3300e+01, 1.3900e+01, 9.6000e+00, 2.6000e+00, 0.0000e+00,\n",
      "        1.4200e+01, 9.4000e+00, 5.9000e+00, 1.1100e+01, 2.1000e+00,\n",
      "        4.3000e+00, 8.0000e-01, 1.5300e+01, 5.5000e+00, 1.0700e+01,\n",
      "        7.0000e+00, 1.3900e+01, 6.5000e+00, 2.7000e+00, 6.5000e+00,\n",
      "        1.1400e+01, 1.4300e+01, 9.9000e+00, 2.5000e+00, 1.1800e+01,\n",
      "        5.8000e+00, 4.1000e+00, 1.0000e+00, 6.9000e+00, 3.3000e+00,\n",
      "        4.8000e+00, 9.6000e+00, 1.3000e+01, 4.7000e+00, 7.9000e+00],\n",
      "       [1.4800e-01, 8.2000e-02, 9.6000e-02, 8.2000e-02, 6.4000e-02,\n",
      "        8.2000e-02, 1.2800e-01, 1.7000e-01, 4.2000e-02, 1.0800e-01,\n",
      "        1.3000e-01, 1.4200e-01, 1.4000e-02, 1.0800e-01, 1.2200e-01,\n",
      "        6.2000e-02, 2.0000e-02, 7.4000e-02, 2.3200e-01, 6.4000e-02,\n",
      "        1.3400e-01, 1.0600e-01, 6.0000e-02, 8.2000e-02, 8.6000e-02,\n",
      "        1.3400e-01, 1.0000e-02, 3.8000e-02, 1.8600e-01, 1.3000e-01,\n",
      "        9.8000e-02, 2.0000e-02, 3.1000e-01, 8.4000e-02, 4.0000e-02,\n",
      "        2.6000e-02, 5.4000e-02, 8.2000e-02, 5.4000e-02, 2.0000e-03,\n",
      "        2.3000e-01, 8.8000e-02, 7.0000e-02, 2.4000e-02, 6.0000e-02,\n",
      "        6.0000e-03, 1.1200e-01, 5.0000e-02, 9.2000e-02, 6.4000e-02,\n",
      "        2.3200e-01, 1.1200e-01, 3.6000e-02, 3.6400e-01, 5.0000e-02,\n",
      "        1.6000e-01, 1.0800e-01, 1.9800e-01, 4.4600e-01, 6.0000e-03,\n",
      "        3.1600e-01, 6.8000e-02, 4.8000e-02, 9.8000e-02, 2.0400e-01,\n",
      "        2.6600e-01, 2.6000e-01, 1.7600e-01, 1.8600e-01, 9.2000e-02,\n",
      "        1.2000e-02, 7.2000e-02, 1.5000e-01, 7.6000e-02, 8.4000e-02,\n",
      "        2.4000e-02, 2.0800e-01, 7.4000e-02, 1.1200e-01, 2.3400e-01,\n",
      "        1.3800e-01, 6.0000e-02, 3.0200e-01, 4.0000e-03, 9.0000e-02,\n",
      "        4.6000e-02, 9.0000e-02, 5.8000e-02, 1.3000e-01, 8.4000e-02,\n",
      "        7.2000e-02, 1.6400e-01, 2.0000e-02, 4.2000e-02, 5.8000e-02,\n",
      "        3.0000e-02, 3.2000e-02, 1.2200e-01, 5.8000e-02, 7.0000e-02,\n",
      "        1.1200e-01, 8.4000e-02, 2.0000e-01, 8.6000e-02, 1.2000e-02,\n",
      "        8.6000e-02, 5.8000e-02, 6.0000e-03, 1.4400e-01, 1.9600e-01,\n",
      "        4.8000e-02, 5.8000e-02, 1.5400e-01, 1.3400e-01, 1.1200e-01,\n",
      "        1.4000e-01, 3.3200e-01, 1.7200e-01, 1.7400e-01, 2.7000e-01,\n",
      "        2.6800e-01, 5.4000e-02, 5.0000e-02, 8.6000e-02, 5.4000e-02,\n",
      "        1.4400e-01, 6.0000e-02, 1.8000e-02, 8.0000e-02, 2.4400e-01,\n",
      "        6.4000e-02, 8.6000e-02, 0.0000e+00, 9.4000e-02, 1.9800e-01,\n",
      "        1.6000e-02, 1.1000e-01, 6.8000e-02, 1.6200e-01, 2.0600e-01,\n",
      "        3.5600e-01, 2.6800e-01, 1.7800e-01, 4.2800e-01, 1.9600e-01,\n",
      "        7.6000e-02, 2.3400e-01, 2.4400e-01, 1.0800e-01, 9.0000e-02,\n",
      "        9.8000e-02, 4.2000e-02, 7.4000e-02, 9.2000e-02, 2.4400e-01,\n",
      "        1.5200e-01, 4.6000e-02, 1.8000e-02, 2.2000e-02, 8.8000e-02,\n",
      "        1.1200e-01, 5.8000e-02, 1.4600e-01, 6.8000e-02, 1.7000e-01,\n",
      "        6.6000e-02, 1.2400e-01, 6.2000e-02, 1.0000e-02, 4.4000e-02,\n",
      "        2.4600e-01, 3.6000e-02, 3.2000e-02, 1.7000e-01, 1.5400e-01,\n",
      "        1.6000e-02, 9.2000e-02, 7.0000e-02, 1.6800e-01, 2.2200e-01,\n",
      "        6.6000e-02, 1.0200e-01, 8.6000e-02, 1.8000e-02, 8.8000e-02,\n",
      "        1.5000e-01, 1.4800e-01, 4.0000e-02, 1.4000e-01, 6.6000e-02,\n",
      "        1.1200e-01, 2.3400e-01, 1.6800e-01, 6.2000e-02, 3.2000e-02,\n",
      "        3.6000e-02, 8.0000e-03, 1.4200e-01, 1.5800e-01, 3.2800e-01]]), 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    print(i, sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea332635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = DataLoader(dataset, batch_size=8,shuffle=True) # TODO to fix the dataloader ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b703e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26edda",
   "metadata": {},
   "source": [
    "First LSTM is a bi-directional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567c242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension=128, input_size = 4):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 1)\n",
    "\n",
    "    def forward(self, X, N=200):\n",
    "        ''' N is the legnth of a feature vector'''\n",
    "        lstm_output, _ = self.lstm(X)\n",
    "\n",
    "        out_forward = lstm_output[range(len(lstm_output)), N - 1, :self.dimension]\n",
    "        out_reverse = lstm_output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        X_fea = self.drop(out_reduced)\n",
    "\n",
    "        X_fea = self.fc(X_fea)\n",
    "        X_fea = torch.squeeze(X_fea, 1)\n",
    "        out = torch.sigmoid(X_fea)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1aed49",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd6b43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          criterion = nn.BCELoss(),\n",
    "#           valid_loader = train_iter,\n",
    "          num_epochs = 5,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    eval_every = len(train_iter)\n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for  data in train_iter:  \n",
    "            labels = data['label'].double().to(device)\n",
    "            X  = torch.transpose(data['data'],2,1).to(device)\n",
    "            output = model(X)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            total_correct = 0\n",
    "            total_predicted = 0\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                  for  data in train_iter:           \n",
    "                      labels = data['label'].double().to(device)\n",
    "                      X  = torch.transpose(data['data'],2,1).to(device)\n",
    "                      output = model(X)\n",
    "                      loss = criterion(output, labels)\n",
    "#                       print(output, labels)\n",
    "                      classifications = output.round() #binary accuracy\n",
    "                      correct_predictions = sum(classifications==labels).item()\n",
    "                      total_correct += correct_predictions\n",
    "                      total_predicted += len(labels)                        \n",
    "                      valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(train_iter)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "                \n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f} Accuracy: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              average_train_loss, average_valid_loss, total_correct/total_predicted))\n",
    "                \n",
    "#                 # checkpoint\n",
    "#                 if best_valid_loss > average_valid_loss:\n",
    "#                     best_valid_loss = average_valid_loss\n",
    "#                     save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "#                     save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "#     save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "#     print('Finished Training!')\n",
    "\n",
    "\n",
    "model = LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71831f4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [18/270], Train Loss: 0.6426, Valid Loss: 0.6216 Accuracy: 0.7273\n",
      "Epoch [2/15], Step [36/270], Train Loss: 0.6014, Valid Loss: 0.5831 Accuracy: 0.7273\n",
      "Epoch [3/15], Step [54/270], Train Loss: 0.6017, Valid Loss: 0.5848 Accuracy: 0.7273\n",
      "Epoch [4/15], Step [72/270], Train Loss: 0.5832, Valid Loss: 0.5822 Accuracy: 0.7273\n",
      "Epoch [5/15], Step [90/270], Train Loss: 0.6195, Valid Loss: 0.5814 Accuracy: 0.7273\n",
      "Epoch [6/15], Step [108/270], Train Loss: 0.6237, Valid Loss: 0.6287 Accuracy: 0.7273\n",
      "Epoch [7/15], Step [126/270], Train Loss: 0.6427, Valid Loss: 0.5960 Accuracy: 0.7273\n",
      "Epoch [8/15], Step [144/270], Train Loss: 0.5869, Valid Loss: 0.5784 Accuracy: 0.7273\n",
      "Epoch [9/15], Step [162/270], Train Loss: 0.6142, Valid Loss: 0.5779 Accuracy: 0.7273\n",
      "Epoch [10/15], Step [180/270], Train Loss: 0.6340, Valid Loss: 0.5819 Accuracy: 0.7273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# LR 0.01\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_iter, criterion, num_epochs, best_valid_loss)\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():                    \n\u001b[0;32m     43\u001b[0m   \u001b[38;5;66;03m# validation loop\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m  data \u001b[38;5;129;01min\u001b[39;00m train_iter:           \n\u001b[0;32m     45\u001b[0m       labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m       X  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mPDControlDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir,\n\u001b[0;32m     22\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_label(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m---> 24\u001b[0m features  \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     25\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label}\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\projects\\kotelnikov\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LR 0.01\n",
    "train(model=model.to(device).double(), optimizer=optimizer,train_iter=data_train, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ee8e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6457, 0.6342, 0.6442], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6390, 0.6542, 0.6567], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6437, 0.6311, 0.6260], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6587, 0.6576, 0.6467], dtype=torch.float64) tensor([1., 0., 0.], dtype=torch.float64)\n",
      "tensor([0.6503, 0.6423, 0.6562], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6430, 0.6152, 0.6373], dtype=torch.float64) tensor([1., 0., 0.], dtype=torch.float64)\n",
      "tensor([0.6408, 0.6537, 0.6535], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6405, 0.6306, 0.6690], dtype=torch.float64) tensor([1., 0., 0.], dtype=torch.float64)\n",
      "tensor([0.6342, 0.6542, 0.6483], dtype=torch.float64) tensor([1., 0., 0.], dtype=torch.float64)\n",
      "tensor([0.6440, 0.6490, 0.6519], dtype=torch.float64) tensor([0., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6433, 0.6478, 0.6569], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6330, 0.6679, 0.6464], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6513, 0.6278, 0.6517], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6242, 0.6531, 0.6493], dtype=torch.float64) tensor([0., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6434, 0.6452, 0.6449], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6671, 0.6289, 0.6526], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6414, 0.6429, 0.6347], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6431, 0.6488, 0.6276], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6359, 0.6453, 0.6439], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6395, 0.6450, 0.6380], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6402, 0.6594, 0.6465], dtype=torch.float64) tensor([0., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6395, 0.6282, 0.6394], dtype=torch.float64) tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6468, 0.6384, 0.6446], dtype=torch.float64) tensor([0., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6277, 0.6390, 0.6578], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6522, 0.6618, 0.6606], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6645, 0.6580, 0.6338], dtype=torch.float64) tensor([0., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6373, 0.6417, 0.6431], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6548, 0.6355, 0.6530], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6505, 0.6470, 0.6517], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6472, 0.6408, 0.6690], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6160, 0.6382, 0.6608], dtype=torch.float64) tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6587, 0.6243, 0.6347], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6501, 0.6492, 0.6575], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6451, 0.6403, 0.6617], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6321, 0.6218, 0.6310], dtype=torch.float64) tensor([0., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6298, 0.6546, 0.6471], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6600, 0.6360, 0.6618], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6648, 0.6423, 0.6406], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6542, 0.6185, 0.6348], dtype=torch.float64) tensor([0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6460, 0.6690, 0.6489], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6461, 0.6483, 0.6675], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6535, 0.6666, 0.6366], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6600, 0.6563, 0.6432], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6299, 0.6387, 0.6563], dtype=torch.float64) tensor([1., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6367, 0.6313, 0.6395], dtype=torch.float64) tensor([0., 1., 0.], dtype=torch.float64)\n",
      "tensor([0.6714, 0.6256, 0.6478], dtype=torch.float64) tensor([1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0.6477, 0.6650, 0.6323], dtype=torch.float64) tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.6283, 0.6461], dtype=torch.float64) tensor([0., 1.], dtype=torch.float64)\n",
      "Epoch [1/1], Step [48/48], Train Loss: 0.6295, Valid Loss: 0.5990 Accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# LR 0.001\n",
    "train(model=model.to(device).double(), optimizer=optimizer,train_iter=data_train, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67a5a4",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "    The simplistic intial model doesn't work on our data. It can be the model itself, the fact that the data are not normalized, and maybe the fact that the dataset is not normally distributed. Next step is to try a model without a bi-directional approach, but simply a one-dir LSTM for classification. It is nevertheless very close to the current model. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d80c8",
   "metadata": {},
   "source": [
    "Here is an attempt to run the same model with data sampled more uniformly, the  train_iter compensates for under represented class. This way it seems to work, but since it is using same training and evaluation data, the results are non-significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48f296",
   "metadata": {},
   "source": [
    "Let's try to add oversampling for the minority class now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99df3e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;241m39\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m39\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m104\u001b[39m), \u001b[38;5;241m104\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m39\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m104\u001b[39m)] \n\u001b[0;32m      4\u001b[0m sampler \u001b[38;5;241m=\u001b[39m WeightedRandomSampler(weights\u001b[38;5;241m=\u001b[39msample_weights,num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset), replacement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m DataLoader(dataset, sampler \u001b[38;5;241m=\u001b[39m  sampler,  batch_size\u001b[38;5;241m=\u001b[39mbatch_size, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m(\u001b[43mtrain_sampler\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;66;03m# TODO to fix the dataloader ... \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sampler' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "batch_size = 8\n",
    "sample_weights = [ 39/(39+104), 104/(39+104)] \n",
    "sampler = WeightedRandomSampler(weights=sample_weights,num_samples=len(dataset), replacement = True)\n",
    "train_iter = DataLoader(dataset, sampler =  sampler,  batch_size=batch_size) # TODO to fix the dataloader ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bee49a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'val_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'val_iter'"
     ]
    }
   ],
   "source": [
    "model = LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model=model.to(device).double(), optimizer=optimizer,train_iter=train_iter, num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4177ce",
   "metadata": {},
   "source": [
    "When we do the balancing, we manage to overtrain the model. It does not mean too much, however, it is already a better result than previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983c5e1",
   "metadata": {},
   "source": [
    "## Vanilla 2\n",
    "The frist simple model didn't work without dataset balancing, so I am trying a model even simpler one from [here](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d909dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaParkinsonNet(nn.Module):\n",
    "    def __init__(self, output_size, feat_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(VanillaParkinsonNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.feat_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba10571",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "feat_size = 4\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "batch_size = 8\n",
    "train_iter = DataLoader(dataset, batch_size=batch_size,shuffle=True) # TODO to fix the dataloader ... \n",
    "\n",
    "model = VanillaParkinsonNet(output_size,feat_size, hidden_dim, n_layers)\n",
    "model.to(device).double()\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "366e8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10... Step: 100... Loss: 0.000038... Val Loss: 0.000038\n",
      "Validation loss decreased (inf --> 0.000038).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):   \n",
    "    for  data in train_iter:  \n",
    "        labels = data['label'].double()\n",
    "        inputs = torch.transpose(data['data'],2,1)\n",
    "        h = model.init_hidden(len(labels))\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for  data in train_iter:  \n",
    "                lab = data['label'].double()\n",
    "                inp  = torch.transpose(data['data'],2,1)\n",
    "                # Attention - I reinit this hidden for each batch\n",
    "                val_h = model.init_hidden(len(lab))\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f510d6",
   "metadata": {},
   "source": [
    "As previously, we can can overfit the model to the class. The experiment have only shown, that it is possible to overfit an LSTM for our data, and that class balancing is very inportant. Next set of experiments is an attempt to train the model in correct manner, that is to separate the data into training and validation sets.  To start, let's just use random split which does not take into account patient IDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57ac40",
   "metadata": {},
   "source": [
    "## Correct train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9fa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  random_split\n",
    "train_data, test_data = random_split(dataset, [120,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9b5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [37 83]\n"
     ]
    }
   ],
   "source": [
    "# Labels check, check data statistics\n",
    "labels = [0]*len(train_data)\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[i]\n",
    "    labels[i] = sample['label']\n",
    "values, counts = np.unique(labels, return_counts=True)\n",
    "print(values, counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ac8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to train the first model again! We augment the train_data, but not the test_data\n",
    "batch_size = 8\n",
    "sample_weights = torch.from_numpy(np.array([33/(33+87),])).double()\n",
    "#sampler = WeightedRandomSampler(weights=sample_weights,num_samples=len(train_data), replacement = True)\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size) \n",
    "val_iter = DataLoader(test_data, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96aaec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2750], dtype=torch.float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fae539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1, 0, 1]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 0, 1, 1, 0, 1]\n",
      "[1, 1, 0, 1, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 1, 0, 0, 1, 0, 1]\n",
      "[1, 1, 0, 1, 1, 0, 1, 0]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 1, 0, 0, 1]\n",
      "[1, 0, 1, 1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1, 1, 1, 0]\n",
      "[0 1] [33 87]\n"
     ]
    }
   ],
   "source": [
    "# check balanced dataset\n",
    "labels_ = []\n",
    "for  i, data in enumerate(train_iter):      \n",
    "    labels = data['label'].tolist()\n",
    "    print(labels)\n",
    "    labels_.extend(labels)\n",
    "values, counts = np.unique(labels_, return_counts=True)\n",
    "print(values, counts) \n",
    "#### !!!!!!!!!!!! Random sampling doesn't work for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "687f0534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), len(val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63b268",
   "metadata": {},
   "source": [
    "### Redefine train function, so that it uses val dataloader\n",
    "Also a different loss funciton is used here: [BCEWithLogits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b8024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "loss_train = []\n",
    "accuracy_train = []\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          val_iter,\n",
    "          criterion = nn.BCEWithLogitsLoss(),\n",
    "          num_epochs = 10,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    eval_every = len(train_iter)\n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct_t = 0 \n",
    "        total_predicted_t = 0\n",
    "        for  data in train_iter:  \n",
    "            labels = data['label'].double().to(device)\n",
    "            X  = torch.transpose(data['data'],2,1).to(device)\n",
    "            output = model(X)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss_train.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #accuracy train metric\n",
    "            classifications_t = output.round() #binary accuracy\n",
    "            #print(classifications_t, labels)\n",
    "            correct_predictions_t = sum(classifications_t==labels).item()\n",
    "            total_correct_t += correct_predictions_t\n",
    "            total_predicted_t += len(labels)        \n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            total_correct = 0\n",
    "            total_predicted = 0\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                  for  data in val_iter:           \n",
    "                      labels = data['label'].double().to(device)\n",
    "                      X  = torch.transpose(data['data'],2,1).to(device)\n",
    "                      output = model(X)\n",
    "                      loss = criterion(output, labels)\n",
    "#                     print(output, labels)\n",
    "                      classifications = output.round() #binary accuracy\n",
    "#                      print(classifications, labels)\n",
    "                      correct_predictions = sum(classifications==labels).item()\n",
    "                      total_correct += correct_predictions\n",
    "                      total_predicted += len(labels)                        \n",
    "                      valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(train_iter)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "                \n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f} Accuracy: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              average_train_loss, average_valid_loss, total_correct/total_predicted))\n",
    "                \n",
    "#                 # checkpoint\n",
    "#                 if best_valid_loss > average_valid_loss:\n",
    "#                     best_valid_loss = average_valid_loss\n",
    "#                     save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "#                     save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "#     save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "#     print('Finished Training!')\n",
    "        accuracy_train.append(total_correct_t/total_predicted_t)\n",
    "        print(f'Train accuracy {total_correct_t/total_predicted_t}')\n",
    "\n",
    "model = LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 0., 1., 0., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 0., 1., 1., 1., 0., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 0., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 0., 1., 0., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 0., 1., 1., 1., 0., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 0., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 0., 1., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 0., 0., 1., 0., 1., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0., 1., 1., 1., 1., 0., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([0., 1., 0., 1., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 1., 0., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 1., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 1., 0., 0.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 1., 1., 0., 1., 0.], dtype=torch.float64)\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([0., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Epoch [1/10], Step [15/150], Train Loss: 0.6262, Valid Loss: 0.1020 Accuracy: 0.9130\n",
      "Train accuracy 0.625\n",
      "tensor([0., 0., 0., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 0., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 0., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([0., 1., 0., 1., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 0., 1., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       grad_fn=<RoundBackward0>) tensor([1., 1., 1., 1., 0., 0., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "train(model=model.to(device).double(), optimizer=optimizer,train_iter=train_iter, val_iter=val_iter, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf0775",
   "metadata": {},
   "source": [
    "As before, we overfit for the train data, and validation accuracy is low, and validation loss is only increasing during the training.  Moreover, a single label is predicted for validation, no matter the input data. It is a weird behavior, because in training data, different lables are presented,and here a special loss is used, which takes into account unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bf8dc8",
   "metadata": {},
   "source": [
    "Tips and Tricks for LSTM\n",
    "* It is possible to use TimeDistribted LSTM [Keras implementation](https://stackoverflow.com/questions/47410239/how-to-feed-into-lstm-with-4-dimensional-input)\n",
    "* Explanation about inputs and outputs of LSTM [here](https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch)\n",
    "* Batching and collation. torch.utils.data.DataLoader is an iterator which provides all these features. Parameters used below should be clear. One parameter of interest is collate_fn. You can specify how exactly the samples need to be batched using collate_fn. [Tutorial](https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html).\n",
    "* Unbalanced data. Dataloader with sampling [torch](https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80607852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM tests\n",
    "lstm = nn.LSTM(input_size=4,\n",
    "                            hidden_size=128,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df423eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randn(5, 3, 10)\n",
    "input =  dataset.__getitem__(0)['data']\n",
    "input_torch = torch.from_numpy(input)\n",
    "input_torch = torch.transpose(input_torch, 0, 1)\n",
    "input_torch = torch.unsqueeze(input_torch, dim=0).to(torch.float32)\n",
    "input_torch.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
